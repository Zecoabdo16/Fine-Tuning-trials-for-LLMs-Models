{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":54662,"databundleVersionId":6169864,"sourceType":"competition"},{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":6266221,"sourceType":"datasetVersion","datasetId":3601853},{"sourceId":6402466,"sourceType":"datasetVersion","datasetId":3686746},{"sourceId":6404623,"sourceType":"datasetVersion","datasetId":3693034}],"dockerImageVersionId":30529,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install transformers -q\n!pip3 install datasets evaluate scikit-learn sentencepiece langchain torch_xla[tpuvm] -q\n!pip uninstall tensorflow -y #that's the meme part","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-20T11:59:27.702938Z","iopub.execute_input":"2024-06-20T11:59:27.703563Z","iopub.status.idle":"2024-06-20T12:00:47.221357Z","shell.execute_reply.started":"2024-06-20T11:59:27.703530Z","shell.execute_reply":"2024-06-20T12:00:47.220400Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorboardx 2.6.1 requires protobuf>=4.22.3, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nFound existing installation: tensorflow 2.12.0\nUninstalling tensorflow-2.12.0:\n  Successfully uninstalled tensorflow-2.12.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport datasets\nimport torch\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.distributed.parallel_loader as pl\nfrom langchain.prompts import PromptTemplate\nfrom datasets import Dataset\nfrom dataclasses import dataclass\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorWithPadding\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom tqdm import tqdm\nfrom transformers import logging as hf_logging\n\nos.environ.pop('TPU_PROCESS_ADDRESSES')\nos.environ.pop('CLOUD_TPU_TASK_ID')\nhf_logging.set_verbosity_error()\n\nMAX_INPUT=128\nMODEL = \"/kaggle/input/llama2-7b-hf/Llama2-7b-hf\"","metadata":{"execution":{"iopub.status.busy":"2024-06-20T12:00:47.223185Z","iopub.execute_input":"2024-06-20T12:00:47.223472Z","iopub.status.idle":"2024-06-20T12:01:15.475850Z","shell.execute_reply.started":"2024-06-20T12:00:47.223448Z","shell.execute_reply":"2024-06-20T12:01:15.475044Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}]},{"cell_type":"code","source":"df =pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\ndf['prompt'] = df['prompt'].str.replace('[\"', '').replace('\"]', '')\ndf['response_a'] = df['response_a'].str.replace('[\"', '')\ndf['response_b'] = df['response_b'].str.replace('[\"', '')\ndf['response_a'] = df['response_a'].str.replace('\"]', '')\ndf['response_b'] = df['response_b'].str.replace('\"]', '')\ndata = pd.DataFrame({\n    'prompt': df['prompt'],\n    'A': df['response_a'],\n    'B': df['response_b'],\n    'answer': 'z'\n})\n\ndata['answer'] = df.apply(lambda df: 'response_a' if df['winner_model_a'] == 1 else ('response_b' if df['winner_model_b'] == 1 else 'both_answers'), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T12:11:18.927007Z","iopub.execute_input":"2024-06-20T12:11:18.927353Z","iopub.status.idle":"2024-06-20T12:11:22.169944Z","shell.execute_reply.started":"2024-06-20T12:11:18.927326Z","shell.execute_reply":"2024-06-20T12:11:22.169165Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-06-20T12:11:22.171309Z","iopub.execute_input":"2024-06-20T12:11:22.171582Z","iopub.status.idle":"2024-06-20T12:11:22.182844Z","shell.execute_reply.started":"2024-06-20T12:11:22.171558Z","shell.execute_reply":"2024-06-20T12:11:22.182109Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                                                  prompt  \\\n0      Is it morally right to try to have a certain p...   \n1      What is the difference between marriage licens...   \n2      explain function calling. how would you call a...   \n3      How can I create a test set for a very rare ca...   \n4      What is the best way to travel from Tel-Aviv t...   \n...                                                  ...   \n57472  A simple mnemonic for \\u03c0:\\n\\\"How I wish I ...   \n57473  In python, implement a naive Bayes with gaussi...   \n57474  is it unethical to work on building weapons?\",...   \n57475  If a bait contains 0,0025% bromadiolon then ho...   \n57476  three kids eat three apples in three days, how...   \n\n                                                       A  \\\n0      The question of whether it is morally right to...   \n1      A marriage license is a legal document that al...   \n2      Function calling is the process of invoking or...   \n3      Creating a test set for a very rare category c...   \n4      The best way to travel from Tel Aviv to Jerusa...   \n...                                                  ...   \n57472  Sure, let's break it down:\\n\\n1. \\\"How\\\" has 3...   \n57473  Here is an implementation of a naive Bayes cla...   \n57474  Working on weapons technology raises some ethi...   \n57475  Bromadiolone is a rodenticide which is most of...   \n57476                                          27 apples   \n\n                                                       B        answer  \n0      As an AI, I don't have personal beliefs or opi...    response_a  \n1      A marriage license and a marriage certificate ...    response_b  \n2      Function calling is the process of invoking a ...  both_answers  \n3      When building a classifier for a very rare cat...    response_a  \n4      The best way to travel from Tel-Aviv to Jerusa...    response_b  \n...                                                  ...           ...  \n57472  Here is how that mnemonic represents the digit...    response_a  \n57473  Sure! Here's an implementation of a naive Baye...    response_a  \n57474  It depends on the context. Weapons can be used...    response_a  \n57475  As an AI language model, I do not promote or c...    response_b  \n57476  If three kids eat three apples in three days, ...    response_a  \n\n[57477 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>A</th>\n      <th>B</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Is it morally right to try to have a certain p...</td>\n      <td>The question of whether it is morally right to...</td>\n      <td>As an AI, I don't have personal beliefs or opi...</td>\n      <td>response_a</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What is the difference between marriage licens...</td>\n      <td>A marriage license is a legal document that al...</td>\n      <td>A marriage license and a marriage certificate ...</td>\n      <td>response_b</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>explain function calling. how would you call a...</td>\n      <td>Function calling is the process of invoking or...</td>\n      <td>Function calling is the process of invoking a ...</td>\n      <td>both_answers</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>How can I create a test set for a very rare ca...</td>\n      <td>Creating a test set for a very rare category c...</td>\n      <td>When building a classifier for a very rare cat...</td>\n      <td>response_a</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What is the best way to travel from Tel-Aviv t...</td>\n      <td>The best way to travel from Tel Aviv to Jerusa...</td>\n      <td>The best way to travel from Tel-Aviv to Jerusa...</td>\n      <td>response_b</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57472</th>\n      <td>A simple mnemonic for \\u03c0:\\n\\\"How I wish I ...</td>\n      <td>Sure, let's break it down:\\n\\n1. \\\"How\\\" has 3...</td>\n      <td>Here is how that mnemonic represents the digit...</td>\n      <td>response_a</td>\n    </tr>\n    <tr>\n      <th>57473</th>\n      <td>In python, implement a naive Bayes with gaussi...</td>\n      <td>Here is an implementation of a naive Bayes cla...</td>\n      <td>Sure! Here's an implementation of a naive Baye...</td>\n      <td>response_a</td>\n    </tr>\n    <tr>\n      <th>57474</th>\n      <td>is it unethical to work on building weapons?\",...</td>\n      <td>Working on weapons technology raises some ethi...</td>\n      <td>It depends on the context. Weapons can be used...</td>\n      <td>response_a</td>\n    </tr>\n    <tr>\n      <th>57475</th>\n      <td>If a bait contains 0,0025% bromadiolon then ho...</td>\n      <td>Bromadiolone is a rodenticide which is most of...</td>\n      <td>As an AI language model, I do not promote or c...</td>\n      <td>response_b</td>\n    </tr>\n    <tr>\n      <th>57476</th>\n      <td>three kids eat three apples in three days, how...</td>\n      <td>27 apples</td>\n      <td>If three kids eat three apples in three days, ...</td>\n      <td>response_a</td>\n    </tr>\n  </tbody>\n</table>\n<p>57477 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction: You will be given a prompt with 2 generated responses. Answer the following multiple choice question by giving the best response or reply with both_answers if you think both are equivaly goog. Answer should be one among [response_a, response_b , both_answers]\n### prompt: {prompt}\\n\nresponse_a) {a}\\n\nresponse_b) {b}\\n\n\n\n### Response:\n\"\"\"#at first testing I provied answers to this template which resulted in data leak lol\n\nprompt = PromptTemplate(template=template, input_variables=['prompt', 'a', 'b'])\ndf = data\n\n\n\n\ndata = Dataset.from_pandas(df)\n\ndef plot_sequence_lengths(data, max_length=1024): #filter abnormally long samples\n    sequence_lengths = []\n    keep_indices = []\n    for i, example in enumerate(data):\n        sequence_lengths.append(len(example['prompt']) + len(example['A']) +len(example['B']))\n        if sequence_lengths[i] < max_length:\n            keep_indices.append(i)\n    return keep_indices\n\nkeep_indices_train = plot_sequence_lengths(data)\ndata = data.select(keep_indices_train)\n\n\ndf","metadata":{"execution":{"iopub.status.busy":"2024-06-20T12:11:23.405862Z","iopub.execute_input":"2024-06-20T12:11:23.406175Z","iopub.status.idle":"2024-06-20T12:11:27.557263Z","shell.execute_reply.started":"2024-06-20T12:11:23.406152Z","shell.execute_reply":"2024-06-20T12:11:27.556564Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                                  prompt  \\\n0      Is it morally right to try to have a certain p...   \n1      What is the difference between marriage licens...   \n2      explain function calling. how would you call a...   \n3      How can I create a test set for a very rare ca...   \n4      What is the best way to travel from Tel-Aviv t...   \n...                                                  ...   \n57472  A simple mnemonic for \\u03c0:\\n\\\"How I wish I ...   \n57473  In python, implement a naive Bayes with gaussi...   \n57474  is it unethical to work on building weapons?\",...   \n57475  If a bait contains 0,0025% bromadiolon then ho...   \n57476  three kids eat three apples in three days, how...   \n\n                                                       A  \\\n0      The question of whether it is morally right to...   \n1      A marriage license is a legal document that al...   \n2      Function calling is the process of invoking or...   \n3      Creating a test set for a very rare category c...   \n4      The best way to travel from Tel Aviv to Jerusa...   \n...                                                  ...   \n57472  Sure, let's break it down:\\n\\n1. \\\"How\\\" has 3...   \n57473  Here is an implementation of a naive Bayes cla...   \n57474  Working on weapons technology raises some ethi...   \n57475  Bromadiolone is a rodenticide which is most of...   \n57476                                          27 apples   \n\n                                                       B        answer  \n0      As an AI, I don't have personal beliefs or opi...    response_a  \n1      A marriage license and a marriage certificate ...    response_b  \n2      Function calling is the process of invoking a ...  both_answers  \n3      When building a classifier for a very rare cat...    response_a  \n4      The best way to travel from Tel-Aviv to Jerusa...    response_b  \n...                                                  ...           ...  \n57472  Here is how that mnemonic represents the digit...    response_a  \n57473  Sure! Here's an implementation of a naive Baye...    response_a  \n57474  It depends on the context. Weapons can be used...    response_a  \n57475  As an AI language model, I do not promote or c...    response_b  \n57476  If three kids eat three apples in three days, ...    response_a  \n\n[57477 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>A</th>\n      <th>B</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Is it morally right to try to have a certain p...</td>\n      <td>The question of whether it is morally right to...</td>\n      <td>As an AI, I don't have personal beliefs or opi...</td>\n      <td>response_a</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What is the difference between marriage licens...</td>\n      <td>A marriage license is a legal document that al...</td>\n      <td>A marriage license and a marriage certificate ...</td>\n      <td>response_b</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>explain function calling. how would you call a...</td>\n      <td>Function calling is the process of invoking or...</td>\n      <td>Function calling is the process of invoking a ...</td>\n      <td>both_answers</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>How can I create a test set for a very rare ca...</td>\n      <td>Creating a test set for a very rare category c...</td>\n      <td>When building a classifier for a very rare cat...</td>\n      <td>response_a</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What is the best way to travel from Tel-Aviv t...</td>\n      <td>The best way to travel from Tel Aviv to Jerusa...</td>\n      <td>The best way to travel from Tel-Aviv to Jerusa...</td>\n      <td>response_b</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57472</th>\n      <td>A simple mnemonic for \\u03c0:\\n\\\"How I wish I ...</td>\n      <td>Sure, let's break it down:\\n\\n1. \\\"How\\\" has 3...</td>\n      <td>Here is how that mnemonic represents the digit...</td>\n      <td>response_a</td>\n    </tr>\n    <tr>\n      <th>57473</th>\n      <td>In python, implement a naive Bayes with gaussi...</td>\n      <td>Here is an implementation of a naive Bayes cla...</td>\n      <td>Sure! Here's an implementation of a naive Baye...</td>\n      <td>response_a</td>\n    </tr>\n    <tr>\n      <th>57474</th>\n      <td>is it unethical to work on building weapons?\",...</td>\n      <td>Working on weapons technology raises some ethi...</td>\n      <td>It depends on the context. Weapons can be used...</td>\n      <td>response_a</td>\n    </tr>\n    <tr>\n      <th>57475</th>\n      <td>If a bait contains 0,0025% bromadiolon then ho...</td>\n      <td>Bromadiolone is a rodenticide which is most of...</td>\n      <td>As an AI language model, I do not promote or c...</td>\n      <td>response_b</td>\n    </tr>\n    <tr>\n      <th>57476</th>\n      <td>three kids eat three apples in three days, how...</td>\n      <td>27 apples</td>\n      <td>If three kids eat three apples in three days, ...</td>\n      <td>response_a</td>\n    </tr>\n  </tbody>\n</table>\n<p>57477 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-06-20T12:11:27.558566Z","iopub.execute_input":"2024-06-20T12:11:27.558843Z","iopub.status.idle":"2024-06-20T12:11:27.563840Z","shell.execute_reply.started":"2024-06-20T12:11:27.558821Z","shell.execute_reply":"2024-06-20T12:11:27.563187Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'A', 'B', 'answer'],\n    num_rows: 12179\n})"},"metadata":{}}]},{"cell_type":"code","source":"def format_text(example):\n    text = prompt.format(prompt=example['prompt'], \n                         a=example['A'], \n                         b=example['B'])\n    return {\"text\": text}\n\ndata = data.map(format_text, num_proc = 56)\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/llama2-7b-hf/Llama2-7b-hf\", trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-06-20T12:11:27.564728Z","iopub.execute_input":"2024-06-20T12:11:27.564985Z","iopub.status.idle":"2024-06-20T12:13:16.825731Z","shell.execute_reply.started":"2024-06-20T12:11:27.564963Z","shell.execute_reply":"2024-06-20T12:13:16.824526Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Map (num_proc=56): 100%|██████████| 12179/12179 [01:47<00:00, 113.40 examples/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(data['text'][0])","metadata":{"execution":{"iopub.status.busy":"2024-06-20T12:13:16.828159Z","iopub.execute_input":"2024-06-20T12:13:16.828464Z","iopub.status.idle":"2024-06-20T12:13:16.854593Z","shell.execute_reply.started":"2024-06-20T12:13:16.828435Z","shell.execute_reply":"2024-06-20T12:13:16.853762Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction: You will be given a prompt with 2 generated responses. Answer the following multiple choice question by giving the best response or reply with both_answers if you think both are equivaly goog. Answer should be one among [response_a, response_b , both_answers]\n### prompt: \\\"Bacteria is life on Mars but a heartbeat isn't life on earth?\\\" What is this quote from?\"]\n\nresponse_a) Dune\n\nresponse_b) This quote seems to be referencing the debate around when life begins in the context of abortion. The key points are:\\n\\n- Bacteria being discovered on Mars would be considered evidence of life on Mars by scientists.\\n\\n- However, a fetus with a heartbeat is not considered a life by those who support abortion rights. \\n\\nSo the quote is juxtaposing these two viewpoints to highlight an apparent contradiction or hypocrisy. The quote implies that if bacteria on Mars is life, then a fetus with a heartbeat should also be considered life.\\n\\nThe source of the quote is not clear to me, but it expresses a pro-life perspective on the abortion debate by using this comparison. It may have originated from a pro-life advocate or organization commenting on the issue.\n\n\n\n### Response:\n\n","output_type":"stream"}]},{"cell_type":"code","source":"FLAGS = {'MAX_INPUT': 1024,\n         'LOGGING_STEPS': 10,\n         'NUM_EPOCHS': 2,\n         'BATCH_SIZE': 8,\n          'NUM_STEPS': len(data['text'])}","metadata":{"execution":{"iopub.status.busy":"2024-06-20T12:13:16.855700Z","iopub.execute_input":"2024-06-20T12:13:16.855936Z","iopub.status.idle":"2024-06-20T12:13:16.892334Z","shell.execute_reply.started":"2024-06-20T12:13:16.855915Z","shell.execute_reply":"2024-06-20T12:13:16.891227Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(example):\n    text_tokens = tokenizer(example[\"text\"], truncation=True, max_length=64, padding='max_length').input_ids\n    answer_tokens = tokenizer(example[\"answer\"], truncation=True, max_length=64, padding='max_length').input_ids\n    return {\n        \"input_ids\": text_tokens,\n        \"label\": answer_tokens,\n    }\n\ndata_train = data.map(preprocess_function, batched=False, num_proc=56).remove_columns(['prompt',  'A', 'B', 'answer']) #remove everything except for input_ids and labels","metadata":{"execution":{"iopub.status.busy":"2024-06-20T12:29:17.162205Z","iopub.execute_input":"2024-06-20T12:29:17.162560Z","iopub.status.idle":"2024-06-20T12:29:20.265359Z","shell.execute_reply.started":"2024-06-20T12:29:17.162534Z","shell.execute_reply":"2024-06-20T12:29:20.263713Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Map (num_proc=56): 100%|██████████| 12179/12179 [00:01<00:00, 8690.00 examples/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"!export XLA_USE_BF16=1\ndef train(index, FLAGS):\n    device = xm.xla_device()\n    train_sampler = torch.utils.data.distributed.DistributedSampler(data_train, num_replicas=8, rank=xm.get_ordinal(),\n                                                                    shuffle=True) #this guy is responsible for distributing data across 8 cores\n    training_loader = torch.utils.data.DataLoader(data_train, batch_size=FLAGS['BATCH_SIZE'], collate_fn=DataCollatorWithPadding(tokenizer=tokenizer),\n                                                  sampler=train_sampler)\n\n    xla_train_loader = pl.MpDeviceLoader(training_loader, device)\n    model = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/llama2-7b-hf/Llama2-7b-hf\", torch_dtype=torch.bfloat16).to(device)\n    cnt = 0\n    for param in model.parameters(): #Freezing most of the layers\n        cnt += 1\n        param.requires_grad = True\n        if cnt < 285:\n            param.requires_grad = False\n\n    model.train()\n    num_replicas = 8\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=FLAGS['NUM_STEPS'] * FLAGS['BATCH_SIZE'])\n    print(f\"Initiliazed model and datasets on core {index}\")\n    num_iterations = int(FLAGS['NUM_STEPS'] / FLAGS['BATCH_SIZE'] / num_replicas)\n    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n        for step, data in enumerate(xla_train_loader):\n            optimizer.zero_grad()\n            outputs = model(**data)\n            loss = outputs.loss\n            loss.backward()\n            xm.optimizer_step(optimizer)\n            if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n                xm.master_print(f'Loss: {loss.item()}, {step + 1} steps out of {num_iterations}, LR: {optimizer.param_groups[0][\"lr\"]}')\n            scheduler.step()\n        xm.master_print(f\"Trained for {epoch} epochs out of {FLAGS['NUM_EPOCHS']}\")\n    xm.master_print(\"Waiting for all processes across cores to finish\")\n    xm.rendezvous('init')\n    xm.master_print(\"Saving the model\")\n    xm.save(model.state_dict(), \"tpu-llama.bin\")\nxmp.spawn(train, args=(FLAGS,), start_method='fork')","metadata":{"execution":{"iopub.status.busy":"2024-06-20T12:29:22.587134Z","iopub.execute_input":"2024-06-20T12:29:22.587539Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"E0620 12:29:49.972090602    5418 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-06-20T12:29:49.972071208+00:00\", grpc_status:2}\nE0620 12:29:49.976310317    5476 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-06-20T12:29:49.976290187+00:00\", grpc_status:2}\nE0620 12:29:49.976473641    5487 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2024-06-20T12:29:49.97645459+00:00\"}\nE0620 12:29:49.989651352    5130 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2024-06-20T12:29:49.989617636+00:00\"}\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n/usr/local/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\nLoading checkpoint shards:  50%|█████     | 1/2 [00:21<00:21, 21.47s/it]\nLoading checkpoint shards:  50%|█████     | 1/2 [00:20<00:20, 20.49s/it]\u001b[A\nLoading checkpoint shards:  50%|█████     | 1/2 [00:21<00:21, 21.82s/it]\u001b[A\nLoading checkpoint shards:  50%|█████     | 1/2 [00:19<00:19, 19.96s/it]\u001b[A\nLoading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.66s/it]\u001b[A\n/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nLoading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.64s/it]\n/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nLoading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.94s/it]\n/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nLoading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.06s/it]\n/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n\nLoading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.55s/it]\u001b[A\n\nLoading checkpoint shards: 100%|██████████| 2/2 [00:26<00:00, 13.13s/it]\u001b[A\n\nLoading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.22s/it]\u001b[A\n\nLoading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.52s/it]\u001b[A\n","output_type":"stream"},{"name":"stdout","text":"Initiliazed model and datasets on core 3\n","output_type":"stream"},{"name":"stderr","text":"Exception in thread Thread-18:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 759, in convert_to_tensors\n    tensor = as_tensor(value)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 721, in as_tensor\n    return torch.tensor(value)\nValueError: too many dimensions 'str'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n    self.run()\n  File \"/usr/local/lib/python3.8/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/torch_xla/distributed/parallel_loader.py\", line 140, in _loader_worker\n    _, data = next(data_iter)\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 634, in __next__\n    data = self._next_data()\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 678, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 271, in __call__\n    batch = pad_without_fast_tokenizer_warning(\n  File \"/usr/local/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 66, in pad_without_fast_tokenizer_warning\n    padded = tokenizer.pad(*pad_args, **pad_kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 3380, in pad\n    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 224, in __init__\n    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 775, in convert_to_tensors\n    raise ValueError(\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n","output_type":"stream"},{"name":"stdout","text":"Initiliazed model and datasets on core 5\n","output_type":"stream"},{"name":"stderr","text":"Exception in thread Thread-18:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 759, in convert_to_tensors\n    tensor = as_tensor(value)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 721, in as_tensor\n    return torch.tensor(value)\nValueError: too many dimensions 'str'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n    self.run()\n  File \"/usr/local/lib/python3.8/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/torch_xla/distributed/parallel_loader.py\", line 140, in _loader_worker\n    _, data = next(data_iter)\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 634, in __next__\n    data = self._next_data()\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 678, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 271, in __call__\n    batch = pad_without_fast_tokenizer_warning(\n  File \"/usr/local/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 66, in pad_without_fast_tokenizer_warning\n    padded = tokenizer.pad(*pad_args, **pad_kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 3380, in pad\n    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 224, in __init__\n    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 775, in convert_to_tensors\n    raise ValueError(\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n","output_type":"stream"},{"name":"stdout","text":"Initiliazed model and datasets on core 6\n","output_type":"stream"},{"name":"stderr","text":"Exception in thread Thread-18:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 759, in convert_to_tensors\n    tensor = as_tensor(value)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 721, in as_tensor\n    return torch.tensor(value)\nValueError: too many dimensions 'str'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n    self.run()\n  File \"/usr/local/lib/python3.8/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/torch_xla/distributed/parallel_loader.py\", line 140, in _loader_worker\n    _, data = next(data_iter)\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 634, in __next__\n    data = self._next_data()\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 678, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 271, in __call__\n    batch = pad_without_fast_tokenizer_warning(\n  File \"/usr/local/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 66, in pad_without_fast_tokenizer_warning\n    padded = tokenizer.pad(*pad_args, **pad_kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 3380, in pad\n    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 224, in __init__\n    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 775, in convert_to_tensors\n    raise ValueError(\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n","output_type":"stream"},{"name":"stdout","text":"Initiliazed model and datasets on core 2\n","output_type":"stream"},{"name":"stderr","text":"Exception in thread Thread-20:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 759, in convert_to_tensors\n    tensor = as_tensor(value)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 721, in as_tensor\n    return torch.tensor(value)\nValueError: too many dimensions 'str'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n    self.run()\n  File \"/usr/local/lib/python3.8/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/torch_xla/distributed/parallel_loader.py\", line 140, in _loader_worker\n    _, data = next(data_iter)\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 634, in __next__\n    data = self._next_data()\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 678, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 271, in __call__\n    batch = pad_without_fast_tokenizer_warning(\n  File \"/usr/local/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 66, in pad_without_fast_tokenizer_warning\n    padded = tokenizer.pad(*pad_args, **pad_kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 3380, in pad\n    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 224, in __init__\n    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 775, in convert_to_tensors\n    raise ValueError(\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n","output_type":"stream"},{"name":"stdout","text":"Initiliazed model and datasets on core 0\n","output_type":"stream"},{"name":"stderr","text":"Exception in thread Thread-18:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 759, in convert_to_tensors\n    tensor = as_tensor(value)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 721, in as_tensor\n    return torch.tensor(value)\nValueError: too many dimensions 'str'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n    self.run()\n  File \"/usr/local/lib/python3.8/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/torch_xla/distributed/parallel_loader.py\", line 140, in _loader_worker\n    _, data = next(data_iter)\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 634, in __next__\n    data = self._next_data()\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 678, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 271, in __call__\n    batch = pad_without_fast_tokenizer_warning(\n  File \"/usr/local/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 66, in pad_without_fast_tokenizer_warning\n    padded = tokenizer.pad(*pad_args, **pad_kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 3380, in pad\n    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 224, in __init__\n    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 775, in convert_to_tensors\n    raise ValueError(\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n","output_type":"stream"},{"name":"stdout","text":"Initiliazed model and datasets on core 4\n","output_type":"stream"},{"name":"stderr","text":"Exception in thread Thread-20:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 759, in convert_to_tensors\n    tensor = as_tensor(value)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 721, in as_tensor\n    return torch.tensor(value)\nValueError: too many dimensions 'str'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n    self.run()\n  File \"/usr/local/lib/python3.8/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/torch_xla/distributed/parallel_loader.py\", line 140, in _loader_worker\n    _, data = next(data_iter)\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 634, in __next__\n    data = self._next_data()\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 678, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 271, in __call__\n    batch = pad_without_fast_tokenizer_warning(\n  File \"/usr/local/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 66, in pad_without_fast_tokenizer_warning\n    padded = tokenizer.pad(*pad_args, **pad_kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 3380, in pad\n    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 224, in __init__\n    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 775, in convert_to_tensors\n    raise ValueError(\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n","output_type":"stream"},{"name":"stdout","text":"Initiliazed model and datasets on core 1\n","output_type":"stream"},{"name":"stderr","text":"Exception in thread Thread-20:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 759, in convert_to_tensors\n    tensor = as_tensor(value)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 721, in as_tensor\n    return torch.tensor(value)\nValueError: too many dimensions 'str'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n    self.run()\n  File \"/usr/local/lib/python3.8/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/torch_xla/distributed/parallel_loader.py\", line 140, in _loader_worker\n    _, data = next(data_iter)\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 634, in __next__\n    data = self._next_data()\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 678, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 271, in __call__\n    batch = pad_without_fast_tokenizer_warning(\n  File \"/usr/local/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 66, in pad_without_fast_tokenizer_warning\n    padded = tokenizer.pad(*pad_args, **pad_kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 3380, in pad\n    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 224, in __init__\n    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 775, in convert_to_tensors\n    raise ValueError(\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n","output_type":"stream"},{"name":"stdout","text":"Initiliazed model and datasets on core 7\n","output_type":"stream"},{"name":"stderr","text":"Exception in thread Thread-20:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 759, in convert_to_tensors\n    tensor = as_tensor(value)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 721, in as_tensor\n    return torch.tensor(value)\nValueError: too many dimensions 'str'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n    self.run()\n  File \"/usr/local/lib/python3.8/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/torch_xla/distributed/parallel_loader.py\", line 140, in _loader_worker\n    _, data = next(data_iter)\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 634, in __next__\n    data = self._next_data()\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 678, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 271, in __call__\n    batch = pad_without_fast_tokenizer_warning(\n  File \"/usr/local/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 66, in pad_without_fast_tokenizer_warning\n    padded = tokenizer.pad(*pad_args, **pad_kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 3380, in pad\n    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 224, in __init__\n    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n  File \"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 775, in convert_to_tensors\n    raise ValueError(\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}